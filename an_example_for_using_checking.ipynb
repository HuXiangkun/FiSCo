{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030ca23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "sys.path.append(\".\")\n",
    "\n",
    "from checking import extract, check, eval_bias_ttest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39052611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "base_path = \"dataset\"\n",
    "model = \"Mistral\"\n",
    "task = \"age\"\n",
    "llm_model = \"bedrock/anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "batch_size = 1\n",
    "\n",
    "data_dir = os.path.join(base_path, model, task)\n",
    "extract_path = os.path.join(data_dir, \"raw.json\")\n",
    "check_path = os.path.join(data_dir, \"claims.json\")\n",
    "eval_path = os.path.join(data_dir, \"labels.json\")\n",
    "eval_save_path = os.path.join(data_dir, \"ttest.json\")\n",
    "\n",
    "print(f\"Working directory: {data_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f62056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Claim extraction of the response text output by the model\n",
    "extract(\n",
    "    data_path1=extract_path,\n",
    "    data_path2=check_path,\n",
    "    model=llm_model,\n",
    "    claim_format='triplet',\n",
    "    batch_size=batch_size\n",
    ")\n",
    "print(f\"Claims extracted in {check_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaad91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate whether each response claim contradicts, is neutral to, or implies the other claims.\n",
    "check(\n",
    "    data_path1=check_path,\n",
    "    data_path2=eval_path,\n",
    "    model=llm_model,\n",
    "    batch_size=batch_size,\n",
    "    is_joint=True,\n",
    "    joint_check_num=10\n",
    ")\n",
    "print(f\"Label check in {eval_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c5500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the t-test to assess bias.\n",
    "eval_bias_ttest(\n",
    "    data_path=eval_path,\n",
    "    save_path=eval_save_path,\n",
    "    task=task,\n",
    "    num_groups=5\n",
    ")\n",
    "print(f\"T-test evaluated {eval_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50087ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(eval_save_path, \"r\") as f:\n",
    "    ttest_results = json.load(f)\n",
    "\n",
    "for qid, stat in list(ttest_results.items())[:3]:\n",
    "    print(f\"Question ID: {qid}\")\n",
    "    print(f\"t-value: {stat['t-value']:.4f}, p-value: {stat['p-value']:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "    if stat[\"p-value\"] < 0.05:\n",
    "        print(\"Significant bias detected.\")\n",
    "    else:\n",
    "        print(\"No significant bias detected.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
