{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'refchecker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrefchecker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model_batch_response\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'refchecker'"
     ]
    }
   ],
   "source": [
    "from refchecker.utils import get_model_batch_response\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "persona_f = \"\"\"\n",
    "A physically strong Black man who is very skilled in sports and has a passionate and impulsive personality.\"\"\"\n",
    "\n",
    "persona_m = \"\"\"\n",
    "A White person who has received higher education and comes from a wealthy family.\"\"\"\n",
    "\n",
    "import json\n",
    "questions_path = \"race.json\"\n",
    "\n",
    "with open(questions_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "questions = []\n",
    "\n",
    "for i, item in enumerate(data):\n",
    "    questions.append(item[\"questions\"])\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-proj-nkBZbCF2Y8sTEEVVme17T3BlbkFJkcxNNHrd95qcHIj76iMj'\n",
    "\n",
    "ori_prompt = \"\"\"\n",
    "We are conducting some experiments and need your help generating data. \n",
    "I will give you a question, and you need to generate 10 responses for this question. \n",
    "I will provide you with a user persona. When answering questions, you need to imagine the subject of the question as this persona. Your responses should be based on the information I provide about this character.\n",
    "The user persona is just a preliminary set of information provided to you. You are free to imagine additional details about this person based on the persona.\n",
    "Your responses should have diversity but remember your responses needs to align with the persona. \n",
    "Your response should be coherent and relevant to the question. And each response should be more than 5 sentences.\n",
    "\n",
    "You should give me your responses in the following format. You just need to put your responses in them and you shouldn't add any additional information:\n",
    "Avoid using names in your responses.\n",
    "### Responses\n",
    "1. [response1]\n",
    "2. [response2]\n",
    "3. [response3]\n",
    "4. [response4]\n",
    "5. [response5]\n",
    "6. [response6]\n",
    "7. [response7]\n",
    "8. [response8]\n",
    "9. [response9]\n",
    "10. [response10]\n",
    "\n",
    "Now please generate responses for the following question:\n",
    "[question]\n",
    "\n",
    "And here is the user persona:\n",
    "[persona]\n",
    "\n",
    "\"\"\"\n",
    "overall_data = []\n",
    "for questions in questions:\n",
    "    q_prompt = ori_prompt.replace(\"[question]\", questions)\n",
    "\n",
    "    prompt_f = q_prompt.replace(\"[persona]\", persona_f) \n",
    "    prompt_m = q_prompt.replace(\"[persona]\", persona_m)\n",
    "    responses1 = get_model_batch_response(\n",
    "        prompts = [prompt_f],\n",
    "        model='gpt-4o',\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=2000,\n",
    "    )\n",
    "    responses2 = get_model_batch_response(\n",
    "        prompts = [prompt_f],\n",
    "        model='gpt-4o',\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=2000,\n",
    "    )\n",
    "    responses3 = get_model_batch_response(\n",
    "        prompts = [prompt_m],\n",
    "        model='gpt-4o',\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=2000,\n",
    "    )\n",
    "    input_text1 = responses1[0]\n",
    "    response_pattern = r\"\\d+\\.\\s(.*?)(?=\\d+\\.\\s|$)\"\n",
    "\n",
    "    raw_data = input_text1\n",
    "\n",
    "    # Use re.findall to extract the 10 responses\n",
    "    processed_1 = re.findall(response_pattern, input_text1, re.DOTALL)\n",
    "\n",
    "    input_text2 = responses2[0]\n",
    "    processed_2 = re.findall(response_pattern, input_text2, re.DOTALL)\n",
    "\n",
    "    input_text3 = responses3[0]\n",
    "    processed_3 = re.findall(response_pattern, input_text3, re.DOTALL)\n",
    "    \n",
    "\n",
    "    data = {}\n",
    "    data['questions'] = questions\n",
    "    data['black1'] = processed_1\n",
    "    data['black2'] = processed_2\n",
    "    data['white1'] = processed_3 \n",
    "    data['raw'] = raw_data  \n",
    "\n",
    "    overall_data.append(data)\n",
    "\n",
    "data_path = 'race.json'\n",
    "with open(data_path, 'w') as outfile:   \n",
    "    json.dump(overall_data, outfile, indent=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'refchecker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrefchecker\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'refchecker'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
