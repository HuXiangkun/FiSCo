{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "362it [00:39,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "litellm.RateLimitError: BedrockException - {\"message\":\"Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again.\"} [sleep 10 seconds]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "383it [06:06,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from refchecker.utils import get_model_batch_response\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设置API Key\n",
    "# os.environ['OPENAI_API_KEY'] = \n",
    "\n",
    "result = []\n",
    "change = []\n",
    "# 模板\n",
    "prompt_template = \"\"\" \n",
    "I would like you to help me measure the semantic similarity between two pieces of text. \n",
    "I will provide you with 3 texts: reference, text1 and text2. \n",
    "You will need to compare text1 and text2 to the reference text and decide which one is more similar to the reference text. Give me a label and a reason for your choice. Label \"0\" means that text1 is more similar to the reference text, while label \"1\" means that text2 is more similar to the reference text.\n",
    "\n",
    "Use the following format for your output:\n",
    "\n",
    "### Label:\n",
    "[label]\n",
    "### Reason:\n",
    "[Reason]\n",
    "\n",
    "Now please give your assessment of the semantic similarity between the following texts:\n",
    "\n",
    "### reference:\n",
    "[reference]\n",
    "\n",
    "### text1:\n",
    "[text1]\n",
    "\n",
    "### text2:\n",
    "[text2]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "check_template = \"\"\"\n",
    "I would like you to help me check the correctness of someone's judgment on the semantic similarity between two pieces of text.\n",
    "I will provide you with 3 texts: reference, text1 and text2, and the label and reason given by someone else. Label \"0\" means that text1 is more similar to the reference text, while label \"1\" means that text2 is more similar to the reference text.\n",
    "Your task is to decide whether the label and reason are correct or not.\n",
    "Give me your judgment and a reason for your checking. \"True\" means that the label are correct, while \"False\" means that the label are incorrect.\n",
    "\n",
    "Use the following format for your output:\n",
    "\n",
    "### Judgment:\n",
    "True / False\n",
    "\n",
    "### Reason:\n",
    "\n",
    "Now please check the correctness of the following judgment:\n",
    "\n",
    "### reference:\n",
    "[reference]\n",
    "\n",
    "### text1:\n",
    "[text1]\n",
    "\n",
    "### text2:\n",
    "[text2]\n",
    "\n",
    "### Somesone's judgment:\n",
    "[judgment]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 加载数据\n",
    "data_path = 'data/raw.json'\n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for i, d in tqdm(enumerate(data)):\n",
    "    if i<359:\n",
    "        continue    \n",
    "    reference = d['reference']  \n",
    "    text1 = d['text1']\n",
    "    text2 = d['text2']\n",
    "    \n",
    "# 0 -> normal\n",
    "    if random.random() < 0.5:\n",
    "        prompt1 = prompt_template.replace('[text1]', text1).replace('[text2]', text2).replace('[reference]', reference)\n",
    "        prompt2 = check_template.replace('[text1]', text1).replace('[text2]', text2).replace('[reference]', reference)\n",
    "        change.append(0)\n",
    "# 1 -> inverse\n",
    "    else:\n",
    "        prompt1 = prompt_template.replace('[text1]', text2).replace('[text2]', text1).replace('[reference]', reference)\n",
    "        prompt2 = check_template.replace('[text1]', text2).replace('[text2]', text1).replace('[reference]', reference)\n",
    "        change.append(1)    \n",
    "\n",
    "    # 获取第一个文本对的响应\n",
    "    responses = get_model_batch_response(\n",
    "        prompts=[prompt1],\n",
    "        model='bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
    "        temperature=0,\n",
    "        max_new_tokens=500\n",
    "    )\n",
    "    response_label = responses[0]\n",
    "    label_match = re.search(r\"### Label:\\n(\\d+)\", response_label)\n",
    "\n",
    "    if label_match:\n",
    "        label_gpt = label_match.group(1)\n",
    "        result.append(label_gpt)\n",
    "    else:\n",
    "        print(i)\n",
    "        with open('data/claude_check.json', 'w') as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        with open('data/claude_change.json', 'w') as f:\n",
    "            json.dump(change, f, indent=2)\n",
    "        assert 0\n",
    "    prompt2 = prompt2.replace('[judgment]', response_label) \n",
    "    # 获取第二个文本对的响应    \n",
    "    responses_check = get_model_batch_response(\n",
    "        prompts=[prompt2],\n",
    "        model='bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
    "        temperature=0,\n",
    "        max_new_tokens=500\n",
    "    )\n",
    "    response_check = responses_check[0] \n",
    "    label_match = re.search(r\"### Judgment:\\n(\\d+)\", response_check)\n",
    "    if label_match:\n",
    "        check_gpt = label_match.group(1)\n",
    "        if not check_gpt:\n",
    "            result[-1]=\"2\"  \n",
    "\n",
    "# 保存结果到json文件\n",
    "with open('data/claude_check.json', 'w') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "with open('data/claude_change.json', 'w') as f:\n",
    "    json.dump(change, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/gpt_score.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/gpt_score.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     gpt_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/raw.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/opt/conda/envs/refchecker/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/gpt_score.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('data/gpt_score.json', 'r') as f:\n",
    "    gpt_data = json.load(f)\n",
    "\n",
    "with open('data/raw.json', 'r') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "correct = 0\n",
    "sum = 0\n",
    "for i, d in enumerate(raw_data):\n",
    "    sum += 1    \n",
    "    d['gpt_score1'] = gpt_data[i][0]\n",
    "    d['gpt_score2'] = gpt_data[i][1]\n",
    "\n",
    "    \n",
    "\n",
    "    if d['gpt_score1'] > d['gpt_score2']:\n",
    "        d['gpt_label'] = 0\n",
    "    elif d['gpt_score1'] < d['gpt_score2']:\n",
    "        d['gpt_label'] = 1\n",
    "    else:\n",
    "        d['gpt_label'] = 2  # equal\n",
    "    \n",
    "    if d['gt_labels'] == d['gpt_label']:\n",
    "        correct += 1\n",
    "    \n",
    "with open('data/gpt_data.json', 'w') as f:\n",
    "    json.dump(raw_data, f, indent=2)\n",
    "\n",
    "print(f'Accuracy: {correct/sum}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7916666666666666\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('data/claude_check.json', 'r') as f:\n",
    "    claude_data = json.load(f)\n",
    "claude_data = [int(x) for x in claude_data] \n",
    "with open('data/claude_change.json', 'r') as f:\n",
    "    change_claude = json.load(f)\n",
    "   \n",
    "# for id,i in enumerate(change_claude):\n",
    "#     if i == 1:\n",
    "#         claude_data[id] = 1 - claude_data[i]\n",
    "\n",
    "with open('data/gpt_check.json', 'r') as f:\n",
    "    gpt_data = json.load(f)\n",
    "gpt_data = [int(x) for x in gpt_data]\n",
    "with open('data/gpt_change.json', 'r') as f:\n",
    "    change_gpt = json.load(f) \n",
    "    \n",
    "for id,i in enumerate(change_gpt):\n",
    "    if i == 1:\n",
    "        gpt_data[id] = 1 - gpt_data[i]\n",
    "\n",
    "def percentage_agreement(a1, a2):\n",
    "    total = len(a1)\n",
    "    agree = sum([1 for x, y in zip(a1, a2) if x == y])\n",
    "    return agree / total\n",
    "\n",
    "print(percentage_agreement(claude_data, gpt_data))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refchecker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
