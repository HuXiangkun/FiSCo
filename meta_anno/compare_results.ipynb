{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/ubuntu/wyw/refchecker_fairness/meta_anno/data/300/gpt_score.json') as f:\n",
    "    gpt_data = json.load(f) \n",
    "\n",
    "with open('/home/ubuntu/wyw/refchecker_fairness/meta_anno/data/300/score.json') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "\n",
    "correct = 0\n",
    "sum = 300\n",
    "for i, d in enumerate(raw_data):\n",
    "    d['gpt_label'] = gpt_data[i]['gpt_label']\n",
    "\n",
    "with open('/home/ubuntu/wyw/refchecker_fairness/meta_anno/data/300/score.json', 'w') as f:\n",
    "    json.dump(raw_data, f, indent=4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.006251433244568265, Accuracy: 0.39316239316239315\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = './data/300/score.json'\n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the range of thresholds to test\n",
    "thresholds = np.logspace(-6, -2, num=10000)  # Example: from 1e-06 to 1e-03 in 100 steps\n",
    "\n",
    "# Initialize variables to track the best threshold\n",
    "best_threshold = None\n",
    "best_accuracy = 0\n",
    "\n",
    "# Loop over each threshold and evaluate performance\n",
    "for threshold in thresholds:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for d in data:\n",
    "        ref_t1 = d['ref_t1_score']\n",
    "        ref_t2 = d['ref_t2_score']\n",
    "        confidence = d['confidence']\n",
    "        gt_labels = d['gpt_label']\n",
    "\n",
    "        if confidence < 0.8:\n",
    "            continue\n",
    "\n",
    "        if abs(ref_t1 - ref_t2) < threshold:\n",
    "            d['rc_labels'] = 2\n",
    "        elif ref_t1 > ref_t2:\n",
    "            d['rc_labels'] = 0\n",
    "        else:\n",
    "            d['rc_labels'] = 1\n",
    "        \n",
    "        total += 1\n",
    "        if d['rc_labels'] == gt_labels:\n",
    "            correct += 1\n",
    "    \n",
    "    # Calculate accuracy for the current threshold\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    # Update the best threshold if the current one is better\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f'Best Threshold: {best_threshold}, Accuracy: {best_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4188034188034188\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load data\n",
    "data_path = './data/300/score.json'\n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Initialize variables\n",
    "threshold = 1e-06\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Process each item in data\n",
    "for d in data:\n",
    "    ref = d['reference']\n",
    "    text1 = d['text1']\n",
    "    text2 = d['text2']\n",
    "    confidences = d['confidence']\n",
    "    gt_labels = d['gpt_label']\n",
    "\n",
    "    # Skip if confidence is too low\n",
    "    if confidences < 0.8:\n",
    "        continue\n",
    "\n",
    "    # Vectorize the texts\n",
    "    vectors = vectorizer.fit_transform([ref, text1, text2])\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cosine_sim_ref_text1 = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]\n",
    "    cosine_sim_ref_text2 = cosine_similarity(vectors[0:1], vectors[2:3])[0][0]\n",
    "\n",
    "    # Determine the label based on cosine similarity and threshold\n",
    "    if abs(cosine_sim_ref_text1 - cosine_sim_ref_text2) < threshold:\n",
    "        d['rc_labels'] = 2\n",
    "    elif cosine_sim_ref_text1 > cosine_sim_ref_text2:\n",
    "        d['rc_labels'] = 0\n",
    "    else:\n",
    "        d['rc_labels'] = 1\n",
    "\n",
    "    # Count correct predictions\n",
    "    total += 1\n",
    "    if d['rc_labels'] == gt_labels:\n",
    "        correct += 1\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy:', correct / total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.41452991452991456\n",
      "Total: 234\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load data\n",
    "data_path = './data/300/score.json'\n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Initialize variables\n",
    "threshold = 1e-06\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Process each item in data\n",
    "for d in data:\n",
    "    ref = d['reference']\n",
    "    text1 = d['text1']\n",
    "    text2 = d['text2']\n",
    "    confidences = d['confidence']\n",
    "    gt_labels = d['gpt_label']\n",
    "\n",
    "    # Skip if confidence is too low\n",
    "    if confidences < 0.8:\n",
    "        continue\n",
    "\n",
    "    # Vectorize the texts\n",
    "    vectors = vectorizer.fit_transform([ref, text1, text2])\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cosine_sim_ref_text1 = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]\n",
    "    cosine_sim_ref_text2 = cosine_similarity(vectors[0:1], vectors[2:3])[0][0]\n",
    "\n",
    "    # Determine the label based on cosine similarity and threshold\n",
    "    if abs(cosine_sim_ref_text1 - cosine_sim_ref_text2) < threshold:\n",
    "        d['rc_labels'] = 2\n",
    "    elif cosine_sim_ref_text1 > cosine_sim_ref_text2:\n",
    "        d['rc_labels'] = 0\n",
    "    else:\n",
    "        d['rc_labels'] = 1\n",
    "\n",
    "    # Count correct predictions\n",
    "    total += 1\n",
    "    if d['rc_labels'] == gt_labels:\n",
    "        correct += 1\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy:', correct / total)\n",
    "print('Total:', total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/refchecker/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4444444444444444\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load data\n",
    "data_path = './data/300/score.json'\n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Load SimCSE model and tokenizer\n",
    "model_name = \"princeton-nlp/sup-simcse-roberta-large\"  # Use appropriate SimCSE model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Initialize variables\n",
    "threshold = 1e-06\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Process each item in data\n",
    "for d in data:\n",
    "    ref = d['reference']\n",
    "    text1 = d['text1']\n",
    "    text2 = d['text2']\n",
    "    confidences = d['confidence']\n",
    "    gt_labels = d['gpt_label']\n",
    "\n",
    "    # Skip if confidence is too low\n",
    "    if confidences < 0.8:\n",
    "        continue\n",
    "\n",
    "    # Compute embeddings\n",
    "    embedding_ref = get_embedding(ref)\n",
    "    embedding_text1 = get_embedding(text1)\n",
    "    embedding_text2 = get_embedding(text2)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_sim_ref_text1 = cosine_similarity(embedding_ref, embedding_text1)[0][0]\n",
    "    cosine_sim_ref_text2 = cosine_similarity(embedding_ref, embedding_text2)[0][0]\n",
    "\n",
    "    # Determine the label based on cosine similarity and threshold\n",
    "    if abs(cosine_sim_ref_text1 - cosine_sim_ref_text2) < threshold:\n",
    "        d['rc_labels'] = 2\n",
    "    elif cosine_sim_ref_text1 > cosine_sim_ref_text2:\n",
    "        d['rc_labels'] = 0\n",
    "    else:\n",
    "        d['rc_labels'] = 1\n",
    "\n",
    "    # Count correct predictions\n",
    "    total += 1\n",
    "    if d['rc_labels'] == gt_labels:\n",
    "        correct += 1\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy:', correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n",
      "Accuracy: 0.4188034188034188\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initialize InferSent model\n",
    "from InferSent.models import InferSent\n",
    "model_version = 1\n",
    "MODEL_PATH = \"./InferSent/encoder/infersent%s.pkl\" % model_version\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "use_cuda = False\n",
    "model = model.cuda() if use_cuda else model\n",
    "\n",
    "W2V_PATH = './InferSent/GloVe/glove.840B.300d.txt' if model_version == 1 else 'fastText/crawl-300d-2M.vec'\n",
    "model.set_w2v_path(W2V_PATH)\n",
    "\n",
    "model.build_vocab_k_words(K=100000)\n",
    "\n",
    "# Initialize variables\n",
    "threshold = 1e-06\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "def get_embedding(text):\n",
    "    sentences = [text]\n",
    "    embeddings = model.encode(sentences, bsize=128, tokenize=False)\n",
    "    return embeddings[0]\n",
    "\n",
    "\n",
    "# Process each item in data\n",
    "for d in data:\n",
    "    ref = d['reference']\n",
    "    text1 = d['text1']\n",
    "    text2 = d['text2']\n",
    "    confidences = d['confidence']\n",
    "    gt_labels = d['gpt_label']\n",
    "\n",
    "    # Skip if confidence is too low\n",
    "    if confidences < 0.8:\n",
    "        continue\n",
    "\n",
    "    # Compute embeddings\n",
    "    embedding_ref = get_embedding(ref)\n",
    "    embedding_text1 = get_embedding(text1)\n",
    "    embedding_text2 = get_embedding(text2)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_sim_ref_text1 = cosine_similarity([embedding_ref], [embedding_text1])[0][0]\n",
    "    cosine_sim_ref_text2 = cosine_similarity([embedding_ref], [embedding_text2])[0][0]\n",
    "\n",
    "    # Determine the label based on cosine similarity and threshold\n",
    "    if abs(cosine_sim_ref_text1 - cosine_sim_ref_text2) < threshold:\n",
    "        d['rc_labels'] = 2\n",
    "    elif cosine_sim_ref_text1 > cosine_sim_ref_text2:\n",
    "        d['rc_labels'] = 0\n",
    "    else:\n",
    "        d['rc_labels'] = 1\n",
    "\n",
    "    # Count correct predictions\n",
    "    total += 1\n",
    "    if d['rc_labels'] == gt_labels:\n",
    "        correct += 1\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy:', correct / total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refchecker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
